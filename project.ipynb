{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### load 'A_Z Handwritten Data.csv' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('A_Z Handwritten Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get copy from the original to preprocess\n",
    "\n",
    "df_pre = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get information about the dataset\n",
    "\n",
    "display(df_pre.describe())\n",
    "\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "\n",
    "# to know the number of the rows\n",
    "print(f\"total records:\",len(df), \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Identify the number of unique classes and show their distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We know that the fisrt column is the target \n",
    "# contains numbers from 0 to 25 (A-Z).\n",
    "\n",
    "# Count the frequency of each letter\n",
    "# df_pre[\"0\"] to get the first column as its name is \"0\" from the above information\n",
    "unique_classes, counts = np.unique(df_pre[\"0\"], return_counts=True)\n",
    "\n",
    "print (\"Unique classes:\", unique_classes)\n",
    "print(\"Unique classes and their counts:\")\n",
    "for cls, count in zip(unique_classes, counts):\n",
    "    print(f\"{chr(cls + ord('A'))}: {count}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# show the distribution\n",
    "plt.bar(unique_classes, counts)\n",
    "plt.title(\"Distribution of Classes\")\n",
    "plt.xlabel(\"Class Labels (A-Z)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Normalize each image. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize images\n",
    "# divide by 255 to make the data between 0 and 1\n",
    "df_normalized = df_pre.astype(\"float32\") / 255.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Reshape the flattened vectors to reconstruct and display the corresponding images while testing the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the target column (first column)\n",
    "# we get it from the original dataset not from the normalized dataset \n",
    "# iloc[Rows , Columns] , \":\" means all records , \"0\" mean the first column \n",
    "df_targets = df_pre.iloc[:, 0].values  \n",
    "\n",
    "# Extract the image data (columns 2 to 785) and reshape each row into 28x28\n",
    "# iloc[Rows , Columns] , \":\" means all records , \"1:\" mean the second column to the last column \n",
    "# the data already is 2d array so the \"images\" will be 3d array \n",
    "# as in each index will contain 2d array ( 28 X 28 ) \n",
    "# \"-1\" automatically calculates the number of images based on the total data size. \n",
    "# mean will return the number of rows\n",
    "df_images = df_normalized.iloc[:, 1:].values.reshape(-1, 28, 28)  \n",
    "\n",
    "\n",
    "print(\"Images shape:\", df_images.shape)\n",
    "\n",
    "print(\"-\"*80)\n",
    "\n",
    "# the firts image in 2d array   \n",
    "print(df_images[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Letters Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have the unique classes and its counts \n",
    "# we need cumulative sum to get index that represents each letter\n",
    "# Create list from the cumulative sum as the following\n",
    "# sum(count[:i]) means sum the counts from the first index to the i index\n",
    "cumulative_counts = [sum(counts[:i]) for i in range(len(counts) + 1)]  \n",
    "\n",
    "\n",
    "# Create a figure for displaying all letters\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Loop through all 26 letters\n",
    "for i, letter in enumerate(unique_classes):\n",
    "    # Get the starting index for the current letter\n",
    "    idx = cumulative_counts[i] + 20\n",
    "\n",
    "    # Create a grid of 4 rows and 7 columns for visualization \n",
    "    plt.subplot(4, 7, i + 1)  \n",
    "    \n",
    "    # Display the image using \"imshow\" that used to display data as an image on a 2D\n",
    "    plt.imshow(df_images[idx], cmap='gray')  \n",
    "\n",
    "    # Show the letter as character not as number\n",
    "    # \"ord\" get the ASCII representation then add the letter number \n",
    "    # then convert to character using casting \"chr\"\n",
    "    plt.title(f\"Letter: {chr(letter + ord('A'))}\") \n",
    "\n",
    "    # Hide axes \n",
    "    plt.axis(\"off\")  \n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Split the data into training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make the 80% from the data training set and 20% from the data testing set\n",
    "# random state to ensure that the split return the same data each run\n",
    "\n",
    "\n",
    "# The final test data will be worked on\n",
    "df_images_train, df_images_test, df_targets_train, df_targets_test = train_test_split(df_images, df_targets, test_size=0.2, random_state=42) \n",
    "\n",
    "\n",
    "\n",
    "# Ensure that the training set and testing set contains all the unique classes\n",
    "\n",
    "targets_train_unique_classes = np.unique(df_targets_train)\n",
    "\n",
    "print(\"No. Of Unique classes in targets train : \\n\" ,len(targets_train_unique_classes) )\n",
    "print(\"-\"*80)\n",
    "\n",
    "targets_test_unique_classes = np.unique(df_targets_test)\n",
    "\n",
    "print(\"No. Of Unique classes in targets test : \\n\" ,len(targets_test_unique_classes) )\n",
    "print(\"-\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">#### Split the training dataset into training and validation datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training dataset into training and validation datasets\n",
    "# random state to ensure that the split return the same data each run\n",
    "\n",
    "\n",
    "# The final training data will be worked on\n",
    "\n",
    "df_images_train_final, df_images_val, df_targets_train_final, df_targets_val = train_test_split(\n",
    "    df_images_train, df_targets_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Ensure that the final training set and validation set contain all unique classes\n",
    "targets_train_final_unique_classes = np.unique(df_targets_train_final)\n",
    "targets_val_unique_classes = np.unique(df_targets_val)\n",
    "\n",
    "targets_train_unique_classes = np.unique(df_targets_train)\n",
    "\n",
    "print(\"No. Of Unique classes in targets train : \\n\" ,len(targets_train_unique_classes) )\n",
    "print(\"-\"*80)\n",
    "\n",
    "targets_test_unique_classes = np.unique(df_targets_test)\n",
    "\n",
    "print(\"No. Of Unique classes in targets test : \\n\" ,len(targets_test_unique_classes) )\n",
    "print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Design 2 Neural Networks (with different number of hidden layers, neurons, activations, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# first neural network model with one hidden layer \n",
    "# the first layer is flatten layer to convert the 2d array to 1d array\n",
    "# the second layer is dense layer with 256 neurons and relu activation function\n",
    "# the last layer is dense layer with 26 neurons(No. Of letters from A to Z) and softmax activation function\n",
    "\n",
    "model_a = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(26 , activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# Set the settings of the model \n",
    "# optimizer is the algorithm that will be used to minimize the loss function\n",
    "# \"adam\" is the most popular optimizer\n",
    "# loss function is the difference between the predicted value and the actual value\n",
    "# \"keras.losses.SparseCategoricalCrossentropy()\" is used for classification problems with multiple classes\n",
    "# metrics is used to evaluate the model performance \n",
    "model_a.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# second neural network model with three hidden layers\n",
    "# the first layer is flatten layer to convert the 2d array to 1d array\n",
    "# the second layer is dense layer with 256 neurons and relu activation function\n",
    "# the third layer is dense layer with 128 neurons and relu activation function\n",
    "# the fourth layer is dense layer with 64 neurons and softplus activation function\n",
    "# the last layer is dense layer with 26 neurons(No. Of letters from A to Z) and softmax activation function\n",
    "\n",
    "model_b = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='softplus'),\n",
    "    tf.keras.layers.Dense(26 , activation='softmax')\n",
    "])\n",
    "\n",
    "model_b.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train each one of these models and plot the error and accuracy curves for the training data and validation datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train the first model on the training data and validate on the validation data\n",
    "# epochs is the number of times the model will be trained on the training data\n",
    "# batch size is the number of samples that will be used in each iteration\n",
    "\n",
    "history_a = model_a.fit(df_images_train_final, df_targets_train_final, validation_data=(df_images_val , df_targets_val),epochs=5 , batch_size=32)\n",
    "\n",
    "# get the loss and accuracy \n",
    "\n",
    "loss_a = history_a.history['loss']\n",
    "val_loss_a = history_a.history['val_loss']\n",
    "accuracy_a = history_a.history['accuracy']\n",
    "val_accuracy_a = history_a.history['val_accuracy']\n",
    "\n",
    "\n",
    "\n",
    "# Plotting Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_a, label='Training Loss')\n",
    "plt.plot(val_loss_a, label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_a, label='Training Accuracy')\n",
    "plt.plot(val_accuracy_a, label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "print(\"-\"*80)\n",
    "\n",
    "\n",
    "# train the second model on the training data and validate on the validation data\n",
    "history_b =  model_b.fit(df_images_train_final, df_targets_train_final, validation_data=(df_images_val , df_targets_val),epochs=5 , batch_size=32)\n",
    "\n",
    "\n",
    "loss_b = history_b.history['loss']\n",
    "val_loss_b = history_b.history['val_loss']\n",
    "accuracy_b = history_b.history['accuracy']\n",
    "val_accuracy_b = history_b.history['val_accuracy']\n",
    "\n",
    "\n",
    "\n",
    "# Plotting Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_b, label='Training Loss')\n",
    "plt.plot(val_loss_b, label='Validation Loss')\n",
    "plt.title('Loss Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(accuracy_b, label='Training Accuracy')\n",
    "plt.plot(val_accuracy_b, label='Validation Accuracy')\n",
    "plt.title('Accuracy Curve')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the best model in a separated file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the validation accuracy of both models\n",
    "\n",
    "# uncomment the following code to compare the validation accuracy of both models and save the best\n",
    "\n",
    "\n",
    "# if max(accuracy_a) > max(accuracy_b):\n",
    "#     best_model = model_a\n",
    "# else:\n",
    "#     best_model = model_b\n",
    "\n",
    "# # Save the best model\n",
    "# best_model.save('best_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Relod the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tf.keras.models.load_model('best_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the best model and provide the confusion matrix and the average f-1 scores for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model on the testing data \n",
    "\n",
    "df_targets_predictions = best_model.predict(df_images_test)\n",
    "\n",
    "\n",
    "# Get the class with the highest probability\n",
    "# argmax return the index of the maximum value\n",
    "# axis=1 to get the maximum value in each row\n",
    "# the result will be the class label (0-25) of the letter\n",
    "df_targets_predictions_classes = np.argmax(df_targets_predictions, axis=1)\n",
    "\n",
    "\n",
    "# Get the confusion matrix \n",
    "cm = confusion_matrix(df_targets_test,df_targets_predictions_classes)\n",
    "\n",
    "# class_labels to represent the letters from A to Z\n",
    "class_labels = [chr(i + ord('A')) for i in range(26)]\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(18, 10))\n",
    "sns.heatmap(cm, \n",
    "            annot=True,         \n",
    "            fmt='g',           \n",
    "            cmap='Blues',       \n",
    "            xticklabels=class_labels,  \n",
    "            yticklabels=class_labels)  \n",
    "\n",
    "plt.ylabel('Actual', fontsize=13)\n",
    "plt.title('Confusion Matrix', fontsize=17, pad=20)\n",
    "plt.gca().xaxis.set_label_position('top')  \n",
    "plt.xlabel('Prediction', fontsize=13)\n",
    "plt.gca().xaxis.tick_top()                \n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provide the average f-1 scores for the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the classification report to get the F1-score avg\n",
    "report = classification_report(df_targets_test, df_targets_predictions_classes, output_dict=True)\n",
    "\n",
    "# get the F1-score for the 'weighted avg'\n",
    "weighted_f1_score = report['weighted avg']['f1-score']\n",
    "\n",
    "# Print the F1-score for the 'weighted avg'\n",
    "print(f\"Weighted Avg F1-score: {weighted_f1_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the best model with images representing the alphabetical letters for the names of each member of your team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of team names to test the best model\n",
    "names = ['AHMED', 'SHAHD', 'SEIF', 'MARYAM']\n",
    "\n",
    "# Test each name\n",
    "for name in names:\n",
    "    # To store the predicted letters of the name\n",
    "    predicted_name = []  \n",
    "    \n",
    "    # Create a subplot for each letter\n",
    "    fig, ax = plt.subplots(1, len(name), figsize=(len(name) * 2, 2))\n",
    "    \n",
    "    # Loop through each letter of the name\n",
    "    for i, letter in enumerate(name):\n",
    "        # Get the image path for each letter\n",
    "        image_path = f'letters/{letter}.png'\n",
    "        \n",
    "        # Load the image and resize it to (28, 28) with grayscale color\n",
    "        img = load_img(image_path, target_size=(28, 28), color_mode='grayscale')\n",
    "\n",
    "        # Normalize the image\n",
    "        img_array = img_to_array(img) / 255.0  \n",
    "\n",
    "        # Reshape to (1, 28, 28)\n",
    "        img_array = np.expand_dims(np.squeeze(img_array), axis=0)  \n",
    "        \n",
    "        # Get predictions from the model\n",
    "        predictions = best_model.predict(img_array)\n",
    "\n",
    "        # Get the class index with the highest probability\n",
    "        predicted_class_index = np.argmax(predictions)  \n",
    "\n",
    "        # Map the class index to the letter\n",
    "        predicted_letter = chr(predicted_class_index + ord('A'))  \n",
    "        \n",
    "        # Append the predicted letter to the predicted name\n",
    "        predicted_name.append(predicted_letter)\n",
    "        \n",
    "        # Visualize the letter\n",
    "        ax[i].imshow(img_array.squeeze(), cmap='gray')\n",
    "        ax[i].axis('off')  \n",
    "        ax[i].set_title(predicted_letter)  \n",
    "    \n",
    "    # Combine the predicted letters to form the full name\n",
    "    predicted_full_name = ''.join(predicted_name)\n",
    "    \n",
    "    # Print the predicted name\n",
    "    print(f\"Predicted name for {name}: {predicted_full_name}\")\n",
    "    \n",
    "    # Show the predicited name with its images\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
